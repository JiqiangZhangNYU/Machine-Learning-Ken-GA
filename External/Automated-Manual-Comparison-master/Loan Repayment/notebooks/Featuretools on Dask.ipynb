{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../images/featuretools.png)\n",
    "\n",
    "# Featuretools Implementation with Dask\n",
    "\n",
    "A calculation of Deep Feature Synthesis from the Automated Loan Repayment notebook running on a single core takes about 25 hours on an AWS EC2 machine. Clearly we need a better approach for practical implementations of calculating a large feature matrix, one that allows us to use all cores of whatever machine we are using. \n",
    "\n",
    "Featuretools does have support for parallel processing if you have multiple cores (which nearly every laptop now does), but it currently sends the entire EntitySet to each process, which means you might exhaust the memory on any one core. For example, the AWS machine has 8 GB per core, which might seem like a lot until you realize the EntitySet takes up about 11 GB and setting `n_jobs=-1` will cause an out of memory error. Therefore, we cannot use the parallel processing in Featuretools and instead have to engineer our own implementation with Dask.  \n",
    "\n",
    "Fortunately, options such as [Dask](https://dask.pydata.org/en/latest/) make it easy to take advantage of multiple cores on our own machine. In this notebook, we'll see how to run Deep Feature Synthesis in about 3 hours on a personal laptop with 8 cores and 16 GB of RAM. \n",
    "\n",
    "![](../../images/dask_logo.png)\n",
    "\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "Following is our plan of action for implementing Dask with Featuretools\n",
    "\n",
    "1. Convert `object` data types to `category`\n",
    "    * This reduces memory consumption significantly\n",
    "2. Create 104 partitions of data and save to disk\n",
    "    * Each partition will contain all data for 1/104 of the clients\n",
    "    * Each partition can be used to __independently__ make an EntitySet and then a feature matrix\n",
    "3. Write a function to take a partition and create an `EntitySet`\n",
    "4. Write a function to take an `EntitySet` and calculate a `feature_matrix` that is saved to disk\n",
    "5. Use Dask to parallelize 3. and 4. to create 104 feature matrices saved on disk\n",
    "6. (Optionally) read in the individual feature matrices and combine into a single feature matrix\n",
    "    \n",
    "The general idea is to __take advantage of all our system resources by breaking one large problem into many smaller ones.__ Each of these smaller problems can be completed on one processor which means we can run multiple (8) of these problems at a time. \n",
    "\n",
    "At the end, we'll have a working implementation of Dask that lets us take full advantage of our computing resources. While a naive approach to this problem would just be renting a larger machine, that won't solve our problem because the bottleneck is not RAM, but using multiple cores at a time. __Often, a better approach than getting more computating resources is to find a way to use the resources we do have as efficiently as possible.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "# Utilities\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data Types\n",
    "\n",
    "The first step is to convert all the data types we can. Using `category` instead of `object` can significantly reduced memory usage if the number of unique categories is much less than the number of observations. For more on the `category` type in Pandas, look at [the documentation.](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n",
    "\n",
    "While this isn't specific to Dask, it's a good practice in general. The function below can be modified for different problems as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    \"\"\"Convert pandas data types for memory reduction.\"\"\"\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif set(df[c].unique()) == {0, 1}:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll read in the datasets and apply the convert types function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory before converting types: 4.38 gb.\n",
      "Total memory after converting types: 2.03 gb.\n"
     ]
    }
   ],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "number_clients = app.shape[0]\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "print(f\"\"\"Total memory before converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Convert types to reduce memory usage\n",
    "app = convert_types(app)\n",
    "bureau = convert_types(bureau)\n",
    "bureau_balance = convert_types(bureau_balance)\n",
    "cash = convert_types(cash)\n",
    "credit = convert_types(credit)\n",
    "previous = convert_types(previous)\n",
    "installments = convert_types(installments)\n",
    "\n",
    "print(f\"\"\"Total memory after converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object memory usage.\n",
      "0.027462848 gb\n",
      "Category memory usage.\n",
      "0.015448612 gb\n",
      "Length of data:  1716428\n",
      "Number of unique categories:  15\n"
     ]
    }
   ],
   "source": [
    "print('Object memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('object').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Category memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('category').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Length of data: ', bureau.shape[0])\n",
    "print('Number of unique categories: ', bureau['CREDIT_TYPE'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the significant difference in memory usage depending on the data type. Since we are looking to get the most from our machine, any step that can reduce computational overhead is beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "\n",
    "Next, we partition the data into 104 separate datasets based on the client id, `SK_ID_CURR` and save the partitions to disk. Every partition will contain the data associated with a subset of the clients and therefore will have 7 smaller csv files. __What is important here is that each partition contains all the data needed to make a feature matrix for the clients and therefore the feature matrix calculations are independent of one another.__\n",
    "\n",
    "* Each partition by itself contains all the data needed to make an `EntitySet` for the clients \n",
    "* This `EntitySet` can then be used to create a feature matrix \n",
    "* Partitioning and saving the raw data allows for more flexilibilitiy when we create the entity set and feature matrix\n",
    "\n",
    "104 partitions was chosen after some trial and error guided by 4 general ideas:\n",
    "\n",
    "1. We want more tasks than workers\n",
    "2. The tasks must be small enough that they don't exhaust the memory of an individual worker\n",
    "3. More tasks will decrease the variance in completion time for each task\n",
    "4. The number of tasks should be a multiple of the number of workers\n",
    "\n",
    "As with many applications in machine learning, experimentation is often the best way to find what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates and saves a dataset with only the users in `user_list`.\"\"\"\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        if partition % 10 == 0:\n",
    "            print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 104 chunks\n",
    "chunk_size = app.shape[0] // 103\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids in id_list:         356255.\n",
      "Total length of application data: 356255.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)\n",
    "    \n",
    "end = timer()\n",
    "print(f'Partitioning took {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I already had the partitions made, but running the above cell took 1300 seconds (21 minutes) the first time. __\n",
    "\n",
    "We can independently generate the feature matrix for each partition because the partition contains all the data for that group of clients. Moreover, each subset of data is small enough for the feature matrix calculation to fit entirely on one core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature Definitions\n",
    "\n",
    "We already calculated the feature definitions in the Automated Loan Repayment notebook so we can read them in. This avoids the need to have to recalculate the features on each partition. Instead of using `ft.dfs`, if we have the feature names, we can use `ft.calculate_feature_matrix` and pass in the `EntitySet` and the feature names. More importantly, it ensures that we create __the exact same set of features for each partition.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "feature_defs = ft.load_features('../input/features.txt')\n",
    "print(len(feature_defs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature matrix, we'll make 1820 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Types\n",
    "\n",
    "If the Automated notebook, we specified the variable types when adding entities to the entityset. However, since we already properly defined the data types for each column, Featuretools will now infer the correct variable type. For example, while before we have Booleans mapped to integers which would be interpreted as numeric, now the Booleans are represented as Booleans and hence will be correctly inferred by Featuretools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Create EntitySet from Partition \n",
    "\n",
    "The next function takes a single partition of data and make an `EntitySet`. We won't save these entitysets to disk, but instead will keep them in memory while calculating the feature matrices. Therefore, if we want to make any changes to the `EntitySet`, such as adding in interesting values or seed features, we can alter this function and remake the `EntitySet` without having to rewrite all the Entity Sets on disk. Writing the entity sets to disk would be another option if we are sure that they won't ever change. We are not going to change the raw data which is why I choose to save the data partitions to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\n",
    "       Returns a dictionary with the entityset and the number used for saving the feature matrix.\"\"\"\n",
    "    \n",
    "    partition_num = int(path[21:])\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return ({'es': es, 'num': partition_num})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 3458, Columns: 122]\n",
       "    bureau [Rows: 16097, Columns: 17]\n",
       "    previous [Rows: 16204, Columns: 37]\n",
       "    bureau_balance [Rows: 166374, Columns: 4]\n",
       "    cash [Rows: 96632, Columns: 8]\n",
       "    installments [Rows: 129130, Columns: 8]\n",
       "    credit [Rows: 35694, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es1_dict = entityset_from_partition('../input/partitions/p1')\n",
    "es1_dict['es']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function works as intended. The next step is to write a function that can take a single `EntitySet` and the `features` we want to build, and make a feature matrix. (`entityset_from_partition` returns a dictionary with the partition number so we can save the feature matrix based on this number.)\n",
    "\n",
    "# Function to Create Feature Matrix from EntitySet \n",
    "\n",
    "With the entity set and the feature names, generating the feature matrix is a one-liner in Featuretools. Since we are going to use Dask for parallelizing the operation, we'll set the number of jobs to 1. The `chunk_size` is an extremely important parameter, and I'd suggest experimenting with this to find the optimal value. What I've found works best is setting the `chunk_size` to the length of the entire dataset provided it can all fit in memory. The best choice may depend on the exact problem. \n",
    "\n",
    "The last step in the function is to save the feature matrix to disk using the name of the partition of data. Using the `feature_defs` ensures that we create the exact same set of features for each parition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es_dict, feature_defs, return_fm = False):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature definitions. \n",
    "    Saves feature matrix based on partition.\"\"\" \n",
    "    \n",
    "    # Extract the entityset\n",
    "    es = es_dict['es']\n",
    "    \n",
    "    # Calculate the feature matrix and save\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_defs, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].df.shape[0])\n",
    "    \n",
    "    feature_matrix.to_csv('../input/fm/p%d_fm.csv' % es_dict['num'], index = True)\n",
    "    \n",
    "    if return_fm:\n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the function using the entityset from the first partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 1820)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "\n",
    "start = timer()\n",
    "fm1 = feature_matrix_from_entityset(es1_dict, feature_defs, return_fm = True)\n",
    "end = timer()\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing one feature matrix took 82.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Computing one feature matrix took {round(end - start, 2)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We now have both parts needed to go from a data partition on disk to a feature matrix made using 1/104 of the data.__ All we have to do is repeat this operation 104 times and we will have all of our features. Since we have eight cores, we can make eight feature matrices at once (your number may differ). This gets around the fundamental bottleneck in the calculation: __running on a single core is inefficient__ especially when we have 8 available. \n",
    "\n",
    "To actually run this in parallel, we use the Dask library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask to parallelize the calculation of feature matrices. First, we'll import and set up a `Client` using processes, which will create one worker for each core on the machine. The memory limit of each worker will be the total system memory (16 gb) divided by the number of cores (8). \n",
    "\n",
    "Then we'll use the `db.from_sequence` method to create a \"Dask bag\" from the partition paths. A [Dask bag](http://dask.pydata.org/en/latest/bag-overview.html) is just a set of operations that we want to run in parallel. We then `map` the paths to the `entityset_from_partition` function which will create the `EntitySets`. These in turn are `map`ped to the `feature_matrix_from_entityset` to make the `feature_matrix` for one of the 104 partitions. (A map is a method that takes a function and a list of inputs and applys the function to each input).\n",
    "\n",
    "Below we clear the system memory for a full run of Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Free up all system memory\n",
    "gc.enable()\n",
    "del app, bureau, bureau_balance, previous, credit, cash, installments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below starts up 8 workers, each using one of our cores. The memory limit per worker will be the total system memory divided by the number of cores. We use `processes` instead of threads because we doing computationally heavy work and because each calculation is __independent__ meaning the workers do not need to talk with one another. \n",
    "\n",
    "(The issue with Python and threads is that threads share memory - processes do not - and because of the [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) there are few operations that can run in parallel using threads.) For more on the topic of processes, threads, and the global interpreter lock in Python, I recommend [this article](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Use all 8 cores\n",
    "client = Client(processes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:59289': 1,\n",
       " 'tcp://127.0.0.1:59292': 1,\n",
       " 'tcp://127.0.0.1:59295': 1,\n",
       " 'tcp://127.0.0.1:59297': 1,\n",
       " 'tcp://127.0.0.1:59300': 1,\n",
       " 'tcp://127.0.0.1:59301': 1,\n",
       " 'tcp://127.0.0.1:59303': 1,\n",
       " 'tcp://127.0.0.1:59305': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ncores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations of Dask\n",
    "\n",
    "After starting a `Client`, if you have `Bokeh` installed, you can navigate to http://localhost:8787/ to view the status of the workers. Doing this on my machine (8 cores with 16 gb total RAM) gives me:\n",
    "\n",
    "![](../images/process_workers.png)\n",
    "\n",
    "Right now we aren't taxing our system very much! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a list of paths of our partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p1',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p4',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p6',\n",
       " '../input/partitions/p7',\n",
       " '../input/partitions/p8']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/p%d' %  i for i in range(1, 105)]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made the partitions small enough that none of the feature matrices will be too large for an individual worker. \n",
    "\n",
    "The next step is the heart of the code. We create a \"Dask bag\" from the paths, map this to the `EntitySet` creating function, and then map the result to the `feature_matrix` create and save function. The `EntitySet` is never saved and only exists in working memory. The cell below does not actually execute the code, but only creates the `bag` of tasks that Dask will then be able to allocate to our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<map-fea..., npartitions=104>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bag object\n",
    "b = db.from_sequence(paths)\n",
    "\n",
    "# Map entityset function\n",
    "b = b.map(entityset_from_partition)\n",
    "\n",
    "# Map feature matrix function\n",
    "b = b.map(feature_matrix_from_entityset, feature_defs = feature_defs)\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below carries out the computation. Nothing is returned since each feature matrix is saved as a `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process 66950 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Elapsed: 2371.93 seconds.\n"
     ]
    }
   ],
   "source": [
    "overall_start = timer()\n",
    "b.compute()\n",
    "overall_end = timer()\n",
    "\n",
    "print(f\"Total Time Elapsed: {round(overall_end - overall_start, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the task graph in the Bokeh dashboard we can see all the tasks that and the sequence in which they have to be completed. From the structure of this [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph), it's clear that this problem is highly parallizable! In computer science terms, this is referred to as [embarassingly parallel](http://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html) because the tasks do not need to communicate with one another. This also lets us run on processes which do not share memory.\n",
    "\n",
    "![](../images/taskgraph_inprogress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Bokeh installed, you can see system information during the run. \n",
    "\n",
    "For example, we can look at the task stream which shows the workers and all the tasks:\n",
    "\n",
    "![](../images/taskstream_inprogress.png)\n",
    "\n",
    "All 8 of our cores are being utilized with a total of 208 tasks to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profile tab shows the amount of time taken by each operation:\n",
    "\n",
    "![](../images/profile2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the operations complete, the Task Graph updates.\n",
    "\n",
    "![](../images/taskgraph_completing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Final Feature Matrix\n",
    "\n",
    "If we want one final matrix, we can read in the individual feature matrices and join them together. This could be done in Dask using threads, but it just as easily can be done in pure Python with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for feature matrices\n",
    "base = '../input/fm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if 'fm.csv' in p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read in the dataframes and place them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in 104 feature matrices took 141 seconds.\n"
     ]
    }
   ],
   "source": [
    "read_start = timer()\n",
    "fms = [pd.read_csv(path) for path in fm_paths]\n",
    "read_end = timer()\n",
    "\n",
    "print(f'Reading in {len(fms)} feature matrices took {round(read_end - read_start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we concatenate all the dataframes in the list along the first axis - meaning that we add the rows to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Matrix Shape: (356255, 1821)\n"
     ]
    }
   ],
   "source": [
    "concat_start = timer()\n",
    "feature_matrix = pd.concat(fms, axis = 0)\n",
    "concat_end = timer()\n",
    "\n",
    "print('Final Feature Matrix Shape:', feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation time: 13.51 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Concatenation time: {round(concat_end - concat_start, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature matrix is exactly the expected shape: the number of clients in `app` by the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have the feature matrix, you can use the following line to save it to disk. This is now ready for feature selection and modeling! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENTILE(MEAN(credit.AMT_TOTAL_RECEIVABLE))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.CNT_DRAWINGS_ATM_CURRENT))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.CNT_DRAWINGS_CURRENT))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.CNT_DRAWINGS_OTHER_CURRENT))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.CNT_DRAWINGS_POS_CURRENT))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.CNT_INSTALMENT_MATURE_CUM))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.SK_DPD))</th>\n",
       "      <th>PERCENTILE(MEAN(credit.SK_DPD_DEF))</th>\n",
       "      <th>PERCENTILE(COUNT(credit))</th>\n",
       "      <th>PERCENTILE(NUM_UNIQUE(credit.NAME_CONTRACT_STATUS))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116121</td>\n",
       "      <td>7033.5</td>\n",
       "      <td>126000.0</td>\n",
       "      <td>126000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116122</td>\n",
       "      <td>26316.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182681</td>\n",
       "      <td>0.434164</td>\n",
       "      <td>0.450178</td>\n",
       "      <td>0.940573</td>\n",
       "      <td>0.921115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116123</td>\n",
       "      <td>21996.0</td>\n",
       "      <td>454500.0</td>\n",
       "      <td>454500.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116124</td>\n",
       "      <td>27801.0</td>\n",
       "      <td>543735.0</td>\n",
       "      <td>486000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116126</td>\n",
       "      <td>29700.0</td>\n",
       "      <td>462694.5</td>\n",
       "      <td>418500.0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  AMT_ANNUITY  AMT_CREDIT  AMT_GOODS_PRICE  AMT_INCOME_TOTAL  \\\n",
       "0      116121       7033.5    126000.0         126000.0           45000.0   \n",
       "1      116122      26316.0    900000.0         900000.0           81000.0   \n",
       "2      116123      21996.0    454500.0         454500.0           90000.0   \n",
       "3      116124      27801.0    543735.0         486000.0           90000.0   \n",
       "4      116126      29700.0    462694.5         418500.0          216000.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0                        0.0                         0.0   \n",
       "1                        NaN                         NaN   \n",
       "2                        0.0                         0.0   \n",
       "3                        NaN                         NaN   \n",
       "4                        0.0                         0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        NaN                        NaN   \n",
       "2                        0.0                        1.0   \n",
       "3                        NaN                        NaN   \n",
       "4                        0.0                        2.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0                         0.0   \n",
       "1                         NaN   \n",
       "2                         0.0   \n",
       "3                         NaN   \n",
       "4                         0.0   \n",
       "\n",
       "                          ...                          \\\n",
       "0                         ...                           \n",
       "1                         ...                           \n",
       "2                         ...                           \n",
       "3                         ...                           \n",
       "4                         ...                           \n",
       "\n",
       "   PERCENTILE(MEAN(credit.AMT_TOTAL_RECEIVABLE))  \\\n",
       "0                                            NaN   \n",
       "1                                       0.180308   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   PERCENTILE(MEAN(credit.CNT_DRAWINGS_ATM_CURRENT))  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "   PERCENTILE(MEAN(credit.CNT_DRAWINGS_CURRENT))  \\\n",
       "0                                            NaN   \n",
       "1                                       0.173191   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   PERCENTILE(MEAN(credit.CNT_DRAWINGS_OTHER_CURRENT))  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                                NaN     \n",
       "4                                                NaN     \n",
       "\n",
       "   PERCENTILE(MEAN(credit.CNT_DRAWINGS_POS_CURRENT))  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "   PERCENTILE(MEAN(credit.CNT_INSTALMENT_MATURE_CUM))  \\\n",
       "0                                                NaN    \n",
       "1                                           0.182681    \n",
       "2                                                NaN    \n",
       "3                                                NaN    \n",
       "4                                                NaN    \n",
       "\n",
       "   PERCENTILE(MEAN(credit.SK_DPD))  PERCENTILE(MEAN(credit.SK_DPD_DEF))  \\\n",
       "0                              NaN                                  NaN   \n",
       "1                         0.434164                             0.450178   \n",
       "2                              NaN                                  NaN   \n",
       "3                              NaN                                  NaN   \n",
       "4                              NaN                                  NaN   \n",
       "\n",
       "   PERCENTILE(COUNT(credit))  \\\n",
       "0                   0.378253   \n",
       "1                   0.940573   \n",
       "2                   0.378253   \n",
       "3                   0.378253   \n",
       "4                   0.378253   \n",
       "\n",
       "  PERCENTILE(NUM_UNIQUE(credit.NAME_CONTRACT_STATUS))  \n",
       "0                                                NaN   \n",
       "1                                           0.921115   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "[5 rows x 1821 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_matrix.reset_index(inplace = True)\n",
    "# feature_matrix.to_csv('../input/feature_matrix.csv', index = False)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "__Using Dask to run the operations in parallel on all our cores reduced the total time to get the feature matrix from 25 hours to under 3 hours.__ This notebook shows that sometimes instead of always trying to get a bigger machine, we need to think about how to use the resouces we have as efficiently as possible. \n",
    "\n",
    "For this problem, to run it in parallel, we had to break one large problem into several smaller ones as follows:\n",
    "\n",
    "1. Partition the data into subsets based on the clients\n",
    "2. Write a function to generate an `EntitySet` from a partition\n",
    "3. Write a function to create a `feature_matrix` from an `EntitySet`\n",
    "4. Set up Dask to use all 8 cores to make a feature matrix from 8 partitions at once\n",
    "5. Save the resulting feature matrices to disk\n",
    "6. (Optional) read in the individual feature matrices to create one final feature matrix.\n",
    "\n",
    "__Parallel processing allows us to take full advantage of our system's resources and the same framework developed in this notebook can be applied to other data science and machine learning problems as well: break a large problem up into manageable smaller ones. Now, not only can we use Featuretools to automatically generate thousands of relevant features, but we can use Dask to run the calculation in parallel and get the most out of our available system.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
